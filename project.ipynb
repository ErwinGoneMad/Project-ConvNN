{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "# üåä Core Libraries\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# üß† TensorFlow & Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.preprocessing.image import (\n",
    "    ImageDataGenerator,\n",
    "    load_img,\n",
    "    img_to_array,\n",
    ")\n",
    "\n",
    "# üß± Keras Layers\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Conv2D,\n",
    "    BatchNormalization,\n",
    "    MaxPool2D,\n",
    "    Dropout,\n",
    "    Dense,\n",
    "    GlobalAveragePooling2D,\n",
    "    Flatten,\n",
    "    Add,\n",
    ")\n",
    "\n",
    "seed_value = 42\n",
    "\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# üìÇ Explore dataset paths\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mships32\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls ./kaggle/working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration, Analyse, Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finir les chemins des dossiers source et destination\n",
    "source_dir = \"./ships32\"\n",
    "dest_dir = \"./ships32augmented\"\n",
    "\n",
    "# Cr√©er le dossier de destination s'il n'existe pas d√©j√†\n",
    "if os.path.exists(dest_dir):\n",
    "    shutil.rmtree(dest_dir)  # Supprimer s'il existe d√©j√†\n",
    "os.makedirs(dest_dir)\n",
    "\n",
    "# Obtenir la liste des sous-dossiers (classes) dans le dossier source\n",
    "classes = os.listdir(source_dir)\n",
    "\n",
    "# Cr√©er les m√™mes sous-dossiers dans le dossier de destination\n",
    "for class_name in classes:\n",
    "    if os.path.isdir(os.path.join(source_dir, class_name)):\n",
    "        os.makedirs(os.path.join(dest_dir, class_name), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = {}\n",
    "for image, label in train_ds.unbatch():\n",
    "    label_val = label.numpy()  # Get the actual label value\n",
    "    if label_val in class_counts:\n",
    "        class_counts[label_val] += 1\n",
    "    else:\n",
    "        class_counts[label_val] = 1\n",
    "\n",
    "# Plot class distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "classes = list(class_counts.keys())\n",
    "counts = list(class_counts.values())\n",
    "plt.bar(classes, counts)\n",
    "plt.xlabel(\"Boat Class\")\n",
    "plt.ylabel(\"Number of Images\")\n",
    "plt.title(\"Class Distribution in Training Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurer le g√©n√©rateur d'augmentation de donn√©es\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.9, 1.1],\n",
    "    fill_mode=\"nearest\",\n",
    ")\n",
    "\n",
    "# Nombre d'images augment√©es √† g√©n√©rer pour chaque image d'origine\n",
    "num_augmented_per_img = 10\n",
    "\n",
    "# Parcourir chaque classe et chaque image\n",
    "for class_name in classes:\n",
    "    if not os.path.isdir(os.path.join(source_dir, class_name)):\n",
    "        continue\n",
    "\n",
    "    class_path = os.path.join(source_dir, class_name)\n",
    "    dest_class_path = os.path.join(dest_dir, class_name)\n",
    "\n",
    "    # Copier d'abord toutes les images originales\n",
    "    for img_name in os.listdir(class_path):\n",
    "        if img_name.endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            shutil.copy2(img_path, dest_class_path)\n",
    "\n",
    "            # Charger l'image et la pr√©parer pour l'augmentation\n",
    "            img = load_img(img_path)\n",
    "            x = img_to_array(img)\n",
    "            x = x.reshape((1,) + x.shape)\n",
    "\n",
    "            # G√©n√©rer des images augment√©es\n",
    "            i = 0\n",
    "            for batch in datagen.flow(\n",
    "                x,\n",
    "                batch_size=1,\n",
    "                save_to_dir=dest_class_path,\n",
    "                save_prefix=os.path.splitext(img_name)[0] + \"_aug\",\n",
    "                save_format=\"png\",\n",
    "            ):\n",
    "                i += 1\n",
    "                if i >= num_augmented_per_img:\n",
    "                    break\n",
    "\n",
    "# Afficher quelques statistiques\n",
    "original_count = sum(\n",
    "    [\n",
    "        len(os.listdir(os.path.join(source_dir, c)))\n",
    "        for c in classes\n",
    "        if os.path.isdir(os.path.join(source_dir, c))\n",
    "    ]\n",
    ")\n",
    "augmented_count = sum(\n",
    "    [\n",
    "        len(os.listdir(os.path.join(dest_dir, c)))\n",
    "        for c in classes\n",
    "        if os.path.isdir(os.path.join(dest_dir, c))\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Nombre d'images originales: {original_count}\")\n",
    "print(f\"Nombre d'images apr√®s augmentation: {augmented_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ships32augmented/ferry/0\")\n",
    "# Trouver toutes les versions augment√©es de la premi√®re image\n",
    "base_name = \"0_aug\"\n",
    "augmented_images = [\n",
    "    img for img in os.listdir(dest_class_path) if img.startswith(base_name)\n",
    "]\n",
    "\n",
    "# Cr√©er une grille pour afficher les images\n",
    "fig, axes = plt.subplots(3, 4, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Afficher l'image originale\n",
    "original_img = load_img(os.path.join(\"ships32augmented/\", class_name, \"2.jpg\"))\n",
    "axes[0].imshow(original_img)\n",
    "axes[0].set_title(\"Image originale\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Afficher les images augment√©es\n",
    "for idx, img_name in enumerate(augmented_images[:10], 1):\n",
    "    img_path = os.path.join(dest_class_path, img_name)\n",
    "    img = load_img(img_path)\n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].set_title(f\"Augmentation {idx}\")\n",
    "    axes[idx].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"ships32augmented\"\n",
    "train_ds = keras.utils.image_dataset_from_directory(\n",
    "    directory,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=128,\n",
    "    image_size=(32, 32),\n",
    "    seed=123,\n",
    "    validation_split=0.2,\n",
    "    shuffle=True,\n",
    "    subset=\"training\",\n",
    ")\n",
    "val_ds = keras.utils.image_dataset_from_directory(\n",
    "    directory,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=128,\n",
    "    image_size=(32, 32),\n",
    "    seed=123,\n",
    "    validation_split=0.2,\n",
    "    shuffle=True,\n",
    "    subset=\"validation\",\n",
    ")\n",
    "class_names = train_ds.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1.0 / 255)\n",
    "\n",
    "# Application de la normalisation aux datasets\n",
    "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define AUTOTUNE to utilize TensorFlow's automatic optimization\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Cache, shuffle, and prefetch the training dataset for improved performance\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# Cache and prefetch the validation dataset for improved performance\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BoatNet_30(input_shape=(32, 32, 3), num_classes=13):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Block 1\n",
    "    x = Conv2D(\n",
    "        64, (3, 3), padding=\"same\", activation=\"relu\", kernel_regularizer=l2(2e-4)\n",
    "    )(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(\n",
    "        64, (3, 3), padding=\"same\", activation=\"relu\", kernel_regularizer=l2(2e-4)\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = Conv2D(\n",
    "        128, (3, 3), padding=\"same\", activation=\"relu\", kernel_regularizer=l2(2e-4)\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(\n",
    "        128, (3, 3), padding=\"same\", activation=\"relu\", kernel_regularizer=l2(2e-4)\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    # Residual Block\n",
    "    # Shortcut before main path\n",
    "    shortcut = Conv2D(128, (1, 1), padding=\"same\")(x)  # 1\n",
    "    # Main path\n",
    "    res = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(x)  # 2\n",
    "    res = BatchNormalization()(res)  # 3\n",
    "    # Add skip connection\n",
    "    x = Add()([shortcut, res])\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(\n",
    "        256, (3, 3), padding=\"same\", activation=\"relu\", kernel_regularizer=l2(2e-4)\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(\n",
    "        256, (3, 3), padding=\"same\", activation=\"relu\", kernel_regularizer=l2(2e-4)\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(\n",
    "        512, (3, 3), padding=\"same\", activation=\"relu\", kernel_regularizer=l2(2e-4)\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "\n",
    "    # Classification\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(768, activation=\"relu\", kernel_regularizer=l2(2e-4))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(num_classes)(x)  # from_logits=True for sparse loss\n",
    "\n",
    "    return Model(inputs=inputs, outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er le mod√®le\n",
    "model = BoatNet_30()\n",
    "\n",
    "# Schedule d'apprentissage plus efficace\n",
    "initial_learning_rate = 1e-3\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=5000, decay_rate=0.9, staircase=True\n",
    ")\n",
    "\n",
    "# Optimizer avec momentum\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# Compilation am√©lior√©e\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Print model summary and layer count\n",
    "model.summary()\n",
    "print(f\"Number of layers: {len(model.layers)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized callbacks for better training\n",
    "callbacks = [\n",
    "    # Early stopping plus patient\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "    ),\n",
    "    # R√©duction du LR plus progressive\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.7,  # Moins agressif (0.7 au lieu de 0.5)\n",
    "        patience=5,\n",
    "        min_lr=5e-6,  # try with 9e-6\n",
    "        verbose=1,\n",
    "    ),\n",
    "    # Model checkpointing\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        \"best_ship_model.keras\", save_best_only=True, monitor=\"val_loss\", verbose=1\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Train model with sufficient epochs\n",
    "history = model.fit(\n",
    "    train_ds, validation_data=val_ds, epochs=150, callbacks=callbacks, verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R√©sultat √† soumettre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load(\"./kaggle/input/navires-2025/ships_competition.npz\", allow_pickle=True)[\n",
    "    \"X\"\n",
    "]\n",
    "X_test = X_test.astype(\"float32\") / 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1745326710.092582 25887979 service.cc:152] XLA service 0x142fb4200 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1745326710.092599 25887979 service.cc:160]   StreamExecutor device (0): Host, Default Version\n",
      "2025-04-22 14:58:30.101001: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1745326710.251778 25887979 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 62ms/step\n"
     ]
    }
   ],
   "source": [
    "res = model.predict(X_test).argmax(axis=1)\n",
    "df = pd.DataFrame({\"Category\": res})\n",
    "df.to_csv(\"reco_nav.csv\", index_label=\"Id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id,Category\n",
      "0,5\n",
      "1,3\n",
      "2,8\n",
      "3,6\n",
      "4,5\n",
      "5,0\n",
      "6,1\n",
      "7,1\n",
      "8,3\n"
     ]
    }
   ],
   "source": [
    "!head reco_nav.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Path (<tt>reco_nav.csv</tt>) doesn't exist. It may still be in the process of being generated, or you may have the incorrect path."
      ],
      "text/plain": [
       "/Users/erwinrodrigues/project/Project-ConvNN/kaggle/working/reco_nav.csv"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(r\"./kaggle/working\")\n",
    "from IPython.display import FileLink\n",
    "\n",
    "FileLink(r\"reco_nav.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mships32\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11655307,
     "sourceId": 97843,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
